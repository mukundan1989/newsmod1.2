{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFwKNQOUY4hSk9zlFggp6x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mukundan1989/newsmod1.2/blob/main/NewsModel_1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sWqDvV4wIXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb521c4b-22ef-43a7-b84b-1d7ca8f58fb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total articles retrieved: 23\n",
            "CSV file has been created successfully.\n"
          ]
        }
      ],
      "source": [
        "import http.client\n",
        "import json\n",
        "import time\n",
        "import csv\n",
        "\n",
        "# Initialize connection\n",
        "conn = http.client.HTTPSConnection(\"seeking-alpha.p.rapidapi.com\")\n",
        "\n",
        "# Set headers\n",
        "headers = {\n",
        "    'x-rapidapi-key': \"3cf0736f79mshe60115701a871c4p19c558jsncccfd9521243\",\n",
        "    'x-rapidapi-host': \"seeking-alpha.p.rapidapi.com\"\n",
        "}\n",
        "\n",
        "# Parameters\n",
        "size = 20  # Number of articles per request\n",
        "since_timestamp = 1722470400  # Set your starting timestamp\n",
        "until_timestamp = 1730369089  # Set your ending timestamp\n",
        "\n",
        "# Read symbols from the file\n",
        "with open('/content/symbollist.txt', 'r') as f:\n",
        "    symbols = [line.strip() for line in f.readlines()]  # Read each line as a symbol\n",
        "\n",
        "# Loop through each symbol\n",
        "for symbol in symbols:\n",
        "    print(f\"Processing symbol: {symbol}\")\n",
        "    all_news_data = []  # Reset for each symbol\n",
        "    current_until_timestamp = until_timestamp  # Reset for each symbol\n",
        "\n",
        "    # Loop to paginate through results for the current symbol\n",
        "    while True:\n",
        "        # Request data with current 'since' and 'until' timestamps\n",
        "        conn.request(\"GET\", f\"/news/v2/list-by-symbol?until={current_until_timestamp}&since={since_timestamp}&size={size}&id={symbol}\", headers=headers)\n",
        "        res = conn.getresponse()\n",
        "\n",
        "        # Read the response and try to decode it as JSON\n",
        "        try:\n",
        "            data = json.loads(res.read().decode(\"utf-8\"))\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Failed to decode JSON for {symbol}: {e}\")\n",
        "            break  # Exit the loop for this symbol if there's a JSON error\n",
        "\n",
        "        # Check if there are articles in the response\n",
        "        if not data['data']:\n",
        "            print(f\"No more articles for {symbol}.\")\n",
        "            break  # Exit if no more articles are available\n",
        "\n",
        "        # Add the fetched data to the list\n",
        "        all_news_data.extend(data['data'])\n",
        "\n",
        "        # Update `until_timestamp` for the next batch (adjust by day or specific interval)\n",
        "        current_until_timestamp -= 86400  # Move one day back (86400 seconds in a day)\n",
        "\n",
        "        # Optional: Sleep to avoid rate limits\n",
        "        time.sleep(2)\n",
        "\n",
        "    # Save the data to a CSV file for the current symbol\n",
        "    if all_news_data:\n",
        "        file_name = f\"{symbol.lower()}_news_data.csv\"\n",
        "        with open(file_name, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            fieldnames = ['ID', 'Publish Date', 'Title', 'Author ID', 'Comment Count', 'Primary Tickers', 'Secondary Tickers', 'Image URL']\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "            writer.writeheader()\n",
        "            for item in all_news_data:\n",
        "                writer.writerow({\n",
        "                    'ID': item['id'],\n",
        "                    'Publish Date': item['attributes']['publishOn'],\n",
        "                    'Title': item['attributes']['title'],\n",
        "                    'Author ID': item['relationships']['author']['data']['id'],\n",
        "                    'Comment Count': item['attributes']['commentCount'],\n",
        "                    'Primary Tickers': ', '.join([t['type'] for t in item['relationships']['primaryTickers']['data']]),\n",
        "                    'Secondary Tickers': ', '.join([t['type'] for t in item['relationships']['secondaryTickers']['data']]),\n",
        "                    'Image URL': item['attributes'].get('gettyImageUrl', 'N/A')\n",
        "                })\n",
        "        print(f\"CSV file created: {file_name}\")\n",
        "    else:\n",
        "        print(f\"No data found for {symbol}. Skipping CSV creation.\")\n",
        "\n",
        "print(\"Processing complete for all symbols.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import http.client\n",
        "import csv\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Directory path containing the files\n",
        "content_dir = \"/content/\"\n",
        "\n",
        "# Find all CSV files ending with \"_news_data.csv\"\n",
        "csv_files = glob.glob(os.path.join(content_dir, \"*_news_data.csv\"))\n",
        "\n",
        "# API details\n",
        "api_key = \"42d8e62a6bmsh25ee08e3260a166p1ce1acjsn6f3b6d6f7d55\"\n",
        "api_host = \"seeking-alpha.p.rapidapi.com\"\n",
        "\n",
        "# Initialize HTTP connection\n",
        "conn = http.client.HTTPSConnection(api_host)\n",
        "\n",
        "# Process each file\n",
        "for csv_file_path in csv_files:\n",
        "    print(f\"Processing file: {csv_file_path}\")\n",
        "    try:\n",
        "        # Load the CSV file\n",
        "        df = pd.read_csv(csv_file_path)\n",
        "\n",
        "        # Ensure there is an 'ID' column\n",
        "        if 'ID' not in df.columns:\n",
        "            print(f\"Skipping file {csv_file_path}: 'ID' column not found.\")\n",
        "            continue\n",
        "\n",
        "        # Initialize 'Content' column if it doesn't exist\n",
        "        if 'Content' not in df.columns:\n",
        "            df['Content'] = None\n",
        "\n",
        "        # Loop through each ID and fetch content\n",
        "        for index, row in df.iterrows():\n",
        "            news_id = row['ID']\n",
        "\n",
        "            # Skip if content is already fetched\n",
        "            if pd.notna(row['Content']):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Make API request\n",
        "                headers = {\n",
        "                    'x-rapidapi-key': api_key,\n",
        "                    'x-rapidapi-host': api_host\n",
        "                }\n",
        "                conn.request(\"GET\", f\"/news/get-details?id={news_id}\", headers=headers)\n",
        "                res = conn.getresponse()\n",
        "                data = res.read()\n",
        "\n",
        "                # Decode and store the content\n",
        "                df.at[index, 'Content'] = data.decode('utf-8')\n",
        "                print(f\"Fetched content for ID: {news_id}\")\n",
        "\n",
        "                # Pause to avoid rate limits\n",
        "                time.sleep(1)  # Adjust delay based on API limits if necessary\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching content for ID {news_id} in file {csv_file_path}: {e}\")\n",
        "\n",
        "        # Save updated CSV\n",
        "        df.to_csv(csv_file_path, index=False)\n",
        "        print(f\"Content updated and saved for file: {csv_file_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {csv_file_path}: {e}\")\n",
        "\n",
        "print(\"Processing complete for all files.\")\n"
      ],
      "metadata": {
        "id": "Wm0uujHVzrkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Function to clean HTML tags\n",
        "def clean_html(raw_html):\n",
        "    clean_regex = re.compile('<.*?>')  # Regex to match HTML tags\n",
        "    return re.sub(clean_regex, '', raw_html)\n",
        "\n",
        "# Function to extract article content from the JSON\n",
        "def extract_content(full_data):\n",
        "    try:\n",
        "        # Load the JSON content\n",
        "        data = json.loads(full_data)\n",
        "        content = data.get(\"data\", {}).get(\"attributes\", {}).get(\"content\", \"\")\n",
        "\n",
        "        # Clean HTML from content\n",
        "        cleaned_content = clean_html(content)\n",
        "\n",
        "        # Define potential ending markers\n",
        "        ending_markers = [\"More on\", \"Read more\", \"See also\", \"Learn more\", \"Related articles\"]\n",
        "\n",
        "        # Find the first occurrence of any ending marker and truncate content\n",
        "        for marker in ending_markers:\n",
        "            if marker in cleaned_content:\n",
        "                cleaned_content = cleaned_content.split(marker)[0]\n",
        "                break  # Stop after finding the first marker\n",
        "\n",
        "        return cleaned_content.strip()  # Remove any leading/trailing spaces\n",
        "    except (json.JSONDecodeError, TypeError):\n",
        "        return None  # Return None if parsing fails\n",
        "\n",
        "# Load your data (assumes CSV file for example purposes)\n",
        "input_file = \"/content/nvda_news_data.csv\"  # Replace with your file path\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "# Apply the extraction function to the \"FullData\" column\n",
        "df[\"Extracted\"] = df[\"FullData\"].apply(extract_content)\n",
        "\n",
        "# Save the updated table to a new file\n",
        "output_file = \"updated_table_with_extracted.csv\"\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Processed table saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "sxY2O74LwbG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load the original PhraseBank\n",
        "phrase_bank_path = '/content/FinancialPhraseBank.csv'\n",
        "column_names = ['Strong Positive', 'Positive', 'Neutral', 'Negative', 'Strong Negative']\n",
        "phrase_bank_df = pd.read_csv(phrase_bank_path, sep=',', names=column_names)\n",
        "\n",
        "# Reshape original PhraseBank\n",
        "phrase_bank = []\n",
        "sentiment_mapping = {\n",
        "    'Strong Positive': 'Strong Positive',\n",
        "    'Positive': 'Positive',\n",
        "    'Neutral': 'Neutral',\n",
        "    'Negative': 'Negative',\n",
        "    'Strong Negative': 'Strong Negative'\n",
        "}\n",
        "\n",
        "for sentiment, phrases in phrase_bank_df.items():\n",
        "    for phrase in phrases.dropna():\n",
        "        phrase_bank.append({'Phrase': phrase, 'Sentiment': sentiment_mapping[sentiment]})\n",
        "\n",
        "phrase_bank_df = pd.DataFrame(phrase_bank)\n",
        "\n",
        "# Load the new PhraseBank2\n",
        "phrase_bank2_path = '/content/FinancialPhraseBank2.csv'\n",
        "column_names2 = ['Positive', 'Negative', 'Neutral']\n",
        "phrase_bank2_df = pd.read_csv(phrase_bank2_path, sep=',', names=column_names2)\n",
        "\n",
        "# Reshape PhraseBank2\n",
        "phrase_bank2 = []\n",
        "for sentiment in column_names2:\n",
        "    for phrase in phrase_bank2_df[sentiment].dropna():\n",
        "        phrase_bank2.append({'Phrase': phrase, 'Sentiment': sentiment})\n",
        "\n",
        "phrase_bank2_df = pd.DataFrame(phrase_bank2)\n",
        "\n",
        "# Load the Bing Liu Lexicon\n",
        "bing_liu_path = '/content/bingliulexicon.csv'\n",
        "bingliu_columns = ['Positive', 'Negative']\n",
        "# Specify encoding to handle non-UTF-8 characters\n",
        "bingliu_df = pd.read_csv(bing_liu_path, sep=',', names=bingliu_columns, encoding='ISO-8859-1')\n",
        "\n",
        "# Reshape Bing Liu Lexicon\n",
        "bingliu_lexicon = []\n",
        "for sentiment in bingliu_columns:\n",
        "    for phrase in bingliu_df[sentiment].dropna():\n",
        "        bingliu_lexicon.append({'Phrase': phrase, 'Sentiment': sentiment})\n",
        "\n",
        "bingliu_df = pd.DataFrame(bingliu_lexicon)\n",
        "\n",
        "# Set up FinBERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Helper function to truncate content\n",
        "def truncate_content(content, tokenizer, max_length=512):\n",
        "    inputs = tokenizer(content, max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
        "    return tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
        "\n",
        "# Analyze sentiment with FinBERT\n",
        "def analyze_sentiment(content):\n",
        "    truncated_content = truncate_content(content, tokenizer)\n",
        "    result = sentiment_pipeline(truncated_content)[0]\n",
        "    return result['label'], result['score']\n",
        "\n",
        "# Calculate sentiment matches for a given PhraseBank\n",
        "def get_phrase_sentiment(content, phrase_bank_df):\n",
        "    matched_sentiments = []\n",
        "    for _, row in phrase_bank_df.iterrows():\n",
        "        if re.search(r'\\b' + re.escape(row['Phrase']) + r'\\b', content, re.IGNORECASE):\n",
        "            matched_sentiments.append(row['Sentiment'])\n",
        "    return matched_sentiments if matched_sentiments else [\"None\"]\n",
        "\n",
        "# Process a single row\n",
        "def process_row(row, phrase_bank_df, phrase_bank2_df, bingliu_df):\n",
        "    content = row['Extracted']\n",
        "    if not isinstance(content, str) or not content.strip():\n",
        "        return None, None, None, None, None  # Handle empty or invalid content\n",
        "\n",
        "    # FinBERT analysis\n",
        "    finbert_label, finbert_score = analyze_sentiment(content)\n",
        "\n",
        "    # PhraseBank1 results\n",
        "    phrasebank1_sentiments = \"; \".join(get_phrase_sentiment(content, phrase_bank_df))\n",
        "\n",
        "    # PhraseBank2 results\n",
        "    phrasebank2_sentiments = \"; \".join(get_phrase_sentiment(content, phrase_bank2_df))\n",
        "\n",
        "    # Bing Liu Lexicon results\n",
        "    bingliu_sentiments = \"; \".join(get_phrase_sentiment(content, bingliu_df))\n",
        "\n",
        "    return finbert_label, finbert_score, phrasebank1_sentiments, phrasebank2_sentiments, bingliu_sentiments\n",
        "\n",
        "# Load the input file\n",
        "input_file = '/content/updated_table_with_extracted.csv'\n",
        "output_file = '/content/updated_table_with_scores.csv'\n",
        "\n",
        "data = pd.read_csv(input_file)\n",
        "\n",
        "# Apply the processing function to each row\n",
        "results = data.apply(\n",
        "    lambda row: process_row(row, phrase_bank_df, phrase_bank2_df, bingliu_df), axis=1\n",
        ")\n",
        "\n",
        "# Add columns for FinBERT, PhraseBank1, PhraseBank2, and Bing Liu Lexicon results\n",
        "data['FinBERT Label'], data['FinBERT Score'], data['PhraseBank1 Results'], data['PhraseBank2 Results'], data['BingLiu Results'] = zip(*results)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "data.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Analysis completed. Updated file saved at {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stp99NHAh9Ap",
        "outputId": "84351190-6bc9-41b2-f205-665004595341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis completed. Updated file saved at /content/updated_table_with_scores.csv\n"
          ]
        }
      ]
    }
  ]
}